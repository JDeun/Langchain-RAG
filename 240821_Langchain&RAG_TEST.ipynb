{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, PyPDFLoader, CSVLoader, Docx2txtLoader,\n",
    "    UnstructuredPowerPointLoader, UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader, UnstructuredExcelLoader\n",
    ")\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 키 설정 (실제 사용시에는 환경 변수 등을 통해 안전하게 관리해야 합니다)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역 설정 변수 정의\n",
    "CHUNK_SIZE = 500  # 텍스트를 나눌 때 각 조각의 크기\n",
    "CHUNK_OVERLAP = 50  # 텍스트 조각 간 겹치는 부분의 크기\n",
    "TEMPERATURE = 0.1  # 모델의 창의성 정도 (0에 가까울수록 일관된 답변, 1에 가까울수록 다양한 답변)\n",
    "MAX_TOKENS = 512  # 생성할 텍스트의 최대 길이\n",
    "TOP_P = 0.95  # 샘플링에 사용할 누적 확률의 임계값\n",
    "REPETITION_PENALTY = 1.15  # 반복을 줄이기 위한 페널티 (값이 클수록 반복이 줄어듦)\n",
    "BATCH_SIZE = 1000  # 데이터 처리 시 한 번에 처리할 항목의 수\n",
    "NUM_RETRIEVED_DOCUMENTS = 3  # 질문에 답변할 때 참조할 문서의 수\n",
    "EMBEDDING_MODEL = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"  # 텍스트를 벡터로 변환하는 데 사용할 모델\n",
    "MEMORY_FILE = \"chat_history.json\"  # 대화 기록을 저장할 파일 이름\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # 사용할 OpenAI 모델 이름\n",
    "SEARCH_TYPE = \"similarity\"  # 'similarity' 또는 'mmr'\n",
    "SEARCH_KWARGS = {\"k\": NUM_RETRIEVED_DOCUMENTS}  # 검색 관련 추가 매개변수\n",
    "PROMPT_TEMPLATE_PATH = \"prompts/Genaral_prompt.txt\" # 프롬프트 파일 경로를 전역 변수로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    다양한 형식의 문서를 로드하는 클래스입니다.\n",
    "    지원하는 파일 형식: txt, pdf, csv, xlsx, xls, docx, pptx, html, md\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 파일 확장자와 해당 로더 클래스를 매핑합니다.\n",
    "        self.loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.pdf': PyPDFLoader,\n",
    "            '.csv': CSVLoader,\n",
    "            '.xlsx': UnstructuredExcelLoader,\n",
    "            '.xls': UnstructuredExcelLoader,\n",
    "            '.docx': Docx2txtLoader,\n",
    "            '.pptx': UnstructuredPowerPointLoader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.md': UnstructuredMarkdownLoader\n",
    "        }\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        주어진 경로의 파일 또는 디렉토리에서 문서를 로드합니다.\n",
    "        \n",
    "        :param path: 로드할 파일 또는 디렉토리 경로\n",
    "        :return: 로드된 문서 리스트\n",
    "        :raises ValueError: 경로가 존재하지 않거나 지원하지 않는 경우\n",
    "        \"\"\"\n",
    "        # 경로를 정규화하고 사용자 홈 디렉토리(~)를 확장합니다.\n",
    "        path = os.path.normpath(os.path.expanduser(path))\n",
    "        \n",
    "        # 경로가 존재하는지 확인합니다.\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"경로가 존재하지 않습니다: {path}\")\n",
    "        \n",
    "        # 파일인 경우 단일 파일을 로드합니다.\n",
    "        if os.path.isfile(path):\n",
    "            return self._load_file(path)\n",
    "        # 디렉토리인 경우 디렉토리 내의 모든 파일을 로드합니다.\n",
    "        elif os.path.isdir(path):\n",
    "            return self._load_directory(path)\n",
    "        # 파일도 디렉토리도 아닌 경우 에러를 발생시킵니다.\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 경로입니다: {path}\")\n",
    "\n",
    "\n",
    "    def _load_file(self, file_path):\n",
    "        \"\"\"\n",
    "        단일 파일을 로드합니다.\n",
    "        \n",
    "        :param file_path: 로드할 파일의 경로\n",
    "        :return: 로드된 문서\n",
    "        :raises ValueError: 지원하지 않는 파일 형식인 경우\n",
    "        \"\"\"\n",
    "        # 파일 확장자를 소문자로 변환하여 가져옵니다.\n",
    "        _, ext = os.path.splitext(file_path.lower())\n",
    "        \n",
    "        # 지원하지 않는 파일 형식인 경우 에러를 발생시킵니다.\n",
    "        if ext not in self.loaders:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식입니다: {ext}\")\n",
    "        \n",
    "        # 적절한 로더를 사용하여 파일을 로드합니다.\n",
    "        loader = self.loaders[ext](file_path)\n",
    "        return loader.load()\n",
    "\n",
    "    def _load_directory(self, dir_path):\n",
    "        documents = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    documents.extend(self._load_file(file_path))\n",
    "                except ValueError as e:\n",
    "                    print(f\"경고: {file_path} 로딩 중 오류 발생 - {str(e)}\")\n",
    "        return documents\n",
    "\n",
    "    def _load_directory(self, dir_path):\n",
    "        \"\"\"\n",
    "        디렉토리 내의 모든 지원되는 파일을 로드합니다.\n",
    "        \n",
    "        :param dir_path: 로드할 디렉토리 경로\n",
    "        :return: 로드된 문서 리스트\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        # 디렉토리를 재귀적으로 순회하며 모든 파일을 처리합니다.\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # 각 파일을 로드하여 문서 리스트에 추가합니다.\n",
    "                    documents.extend(self._load_file(file_path))\n",
    "                except ValueError as e:\n",
    "                    # 지원하지 않는 파일 형식 등의 오류 발생 시 경고 메시지를 출력합니다.\n",
    "                    print(f\"경고: {file_path} 로딩 중 오류 발생 - {str(e)}\")\n",
    "        return documents\n",
    "\n",
    "def load_prompt_template(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    프롬프트 템플릿 파일을 로드합니다.\n",
    "    \n",
    "    :param file_path: 프롬프트 템플릿 파일 경로\n",
    "    :return: 로드된 프롬프트 템플릿 문자열\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 파일을 열어 내용을 읽습니다.\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            # 필요한 변수({context}와 {question})가 템플릿에 포함되어 있는지 확인합니다.\n",
    "            if \"{context}\" not in content or \"{question}\" not in content:\n",
    "                raise ValueError(\"프롬프트 템플릿에 {context}와 {question} 변수가 모두 포함되어야 합니다.\")\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        # 파일을 찾을 수 없는 경우 경고 메시지를 출력하고 기본 템플릿을 반환합니다.\n",
    "        print(f\"프롬프트 템플릿 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        return \"\"\"다음 맥락을 사용하여 질문에 답하세요:\n",
    "        {context}\n",
    "\n",
    "        인간: {question}\n",
    "        AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \"\"\"\n",
    "    대화형 AI 챗봇 클래스입니다.\n",
    "    문서를 로드하고 벡터화하여 질문에 답변합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        ChatBot 객체를 초기화합니다.\n",
    "\n",
    "        :param data_path: 로드할 문서 데이터의 경로\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.embeddings = OpenAIEmbeddings()  # 텍스트를 벡터로 변환하는 임베딩 모델\n",
    "        self.vectorstore = None  # 문서 벡터를 저장할 벡터 저장소\n",
    "        self.qa_chain = None  # 질문-답변 체인\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)  # 대화 기록을 저장할 메모리\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        챗봇 시스템을 설정합니다.\n",
    "        문서를 로드하고, 벡터화하여 저장소를 생성하고, QA 체인을 설정합니다.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"데이터 로딩 중...\")\n",
    "            loader = DataLoader()\n",
    "            documents = loader.load(self.data_path)  # 문서 로드\n",
    "\n",
    "            print(\"텍스트 분할 중...\")\n",
    "            # 긴 문서를 작은 청크로 분할\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "            print(\"벡터 저장소 생성 중...\")\n",
    "            # 분할된 텍스트 청크를 벡터화하여 FAISS 인덱스에 저장\n",
    "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "\n",
    "            print(\"QA 체인 설정 중...\")\n",
    "            # OpenAI 언어 모델 초기화\n",
    "            llm = ChatOpenAI(model_name=MODEL_NAME, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)\n",
    "            \n",
    "            # 프롬프트 템플릿 로드 및 설정\n",
    "            prompt_template = load_prompt_template(PROMPT_TEMPLATE_PATH)\n",
    "            PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "            \n",
    "            # ConversationalRetrievalChain 설정\n",
    "            self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=llm,\n",
    "                retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": NUM_RETRIEVED_DOCUMENTS}),\n",
    "                memory=self.memory,\n",
    "                combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    "                chain_type=\"stuff\"  # 'stuff' 방식으로 문서를 처리 (모든 관련 문서를 하나의 컨텍스트로 결합)\n",
    "            )\n",
    "            print(\"설정 완료!\")\n",
    "        except Exception as e:\n",
    "            print(f\"설정 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def ask(self, query: str):\n",
    "        \"\"\"\n",
    "        주어진 질문에 대한 답변을 생성합니다.\n",
    "\n",
    "        :param query: 사용자의 질문\n",
    "        :return: 챗봇의 답변\n",
    "        \"\"\"\n",
    "        if not self.qa_chain:\n",
    "            self.setup()  # QA 체인이 설정되지 않은 경우 설정을 실행\n",
    "        try:\n",
    "            response = self.qa_chain({\"question\": query})  # QA 체인을 사용하여 질문에 답변\n",
    "            return response['answer']\n",
    "        except Exception as e:\n",
    "            print(f\"질문 처리 중 오류 발생: {str(e)}\")\n",
    "            return \"죄송합니다. 질문을 처리하는 중에 오류가 발생했습니다.\"\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"\n",
    "        대화형 인터페이스를 시작합니다.\n",
    "        사용자의 입력을 받아 답변을 생성하고 출력합니다.\n",
    "        \"\"\"\n",
    "        print(\"챗봇이 준비되었습니다. '종료'를 입력하면 대화를 마칩니다.\")\n",
    "        while True:\n",
    "            query = input(\"질문을 입력하세요: \")\n",
    "            if query.lower() == '종료':\n",
    "                break\n",
    "            answer = self.ask(query)\n",
    "            print(\"\\n답변:\", answer, \"\\n\")\n",
    "        print(\"대화를 종료합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇이 준비되었습니다. '종료'를 입력하면 대화를 마칩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 312 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 346 0 (offset 0)\n",
      "Ignoring wrong pointing object 349 0 (offset 0)\n",
      "Ignoring wrong pointing object 350 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "텍스트 분할 중...\n",
      "벡터 저장소 생성 중...\n",
      "QA 체인 설정 중...\n",
      "설정 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyong-eun/Desktop/Dev/Langchain/bible/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변: 맥 환경에서 라마(LLaMA)를 랭체인(Chain)으로 활용하기 위해서는 다음 단계를 따라야 합니다:\n",
      "\n",
      "1. **필수 소프트웨어 설치**:\n",
      "   - Python이 설치되어 있어야 합니다. Python 3.7 이상을 권장합니다.\n",
      "   - 필요한 패키지를 설치하기 위해 `pip`를 사용합니다. 다음 명령어를 터미널에 입력하여 `langchain`과 `torch`를 설치하세요:\n",
      "     ```bash\n",
      "     pip install langchain torch\n",
      "     ```\n",
      "\n",
      "2. **LLaMA 모델 다운로드**:\n",
      "   - LLaMA 모델을 다운로드하려면 Meta의 공식 웹사이트에서 모델을 요청하고, 다운로드 링크를 받아야 합니다. 모델 파일을 로컬 머신에 저장합니다.\n",
      "\n",
      "3. **모델 로드 및 설정**:\n",
      "   - Python 스크립트에서 LLaMA 모델을 로드합니다. 예를 들어, 다음과 같은 코드를 사용할 수 있습니다:\n",
      "     ```python\n",
      "     from langchain.llms import Llama\n",
      "\n",
      "     # 모델 경로를 지정합니다.\n",
      "     model_path = \"path/to/llama/model\"\n",
      "     llama_model = Llama(model_path=model_path)\n",
      "     ```\n",
      "\n",
      "4. **랭체인 설정**:\n",
      "   - 랭체인을 설정하여 LLaMA 모델을 사용할 수 있도록 합니다. 예를 들어, 다음과 같이 간단한 체인을 만들 수 있습니다:\n",
      "     ```python\n",
      "     from langchain.chains import SimpleChain\n",
      "\n",
      "     chain = SimpleChain(llm=llama_model)\n",
      "     ```\n",
      "\n",
      "5. **질문 및 응답**:\n",
      "   - 이제 체인을 사용하여 질문을 하고 응답을 받을 수 있습니다. 예를 들어:\n",
      "     ```python\n",
      "     response = chain.run(\"당신의 질문을 여기에 입력하세요.\")\n",
      "     print(response)\n",
      "     ```\n",
      "\n",
      "6. **추가 설정 및 최적화**:\n",
      "   - 필요에 따라 추가적인 설정이나 최적화를 진행할 수 있습니다. 예를 들어, 모델의 파라미터를 조정하거나, 다양한 체인 구조를 실험해 볼 수 있습니다.\n",
      "\n",
      "이 과정을 통해 맥 환경에서 LLaMA를 랭체인으로 활용할 수 있습니다. 추가적인 질문이 있으시면 언제든지 말씀해 주세요! \n",
      "\n",
      "\n",
      "답변: 라마는 한국어를 지원합니다. 다양한 언어로 질문을 하실 수 있으며, 한국어로도 원활하게 대화할 수 있습니다. 궁금한 점이 있으시면 언제든지 질문해 주세요! \n",
      "\n",
      "대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_path = input(\"데이터 파일 또는 폴더 경로를 입력하세요: \")\n",
    "    try:\n",
    "        chatbot = ChatBot(data_path)\n",
    "        chatbot.chat()\n",
    "    except ValueError as e:\n",
    "        print(f\"오류 발생: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
